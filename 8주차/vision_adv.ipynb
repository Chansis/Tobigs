{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "LXw1gssjz9yj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# 코드 다시 돌리기 위한 seed 고정\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "PMUR2HB6z_xc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=8,\n",
    "            kernel_size=3,\n",
    "            padding=1)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=16,\n",
    "            kernel_size=3,\n",
    "            padding=1)\n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "        self.fc1 = nn.Linear(8 * 8 * 16, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 8 * 8 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "MyModel(\n  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=1024, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=32, bias=True)\n  (fc3): Linear(in_features=32, out_features=10, bias=True)\n)"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel().to(device)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wccqxwrh0B3U",
    "outputId": "81583de2-ba0b-4dda-bed7-665e233bac66",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train = torchvision.datasets.CIFAR100(root=\"./\", train=True, download=True, transform=train_transform)\n",
    "test = torchvision.datasets.CIFAR100(root=\"./\", train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=256,\n",
    "                                           shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=256,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/170498071 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa401338186f4a0faccfb7406cc886b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=\"./data/\",\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=transforms.ToTensor())\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=\"./data/\",\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/196 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 84 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[80], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     11\u001B[0m output \u001B[38;5;241m=\u001B[39m model(img)\n\u001B[1;32m---> 12\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     14\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m   1173\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m-> 1174\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1175\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1176\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\functional.py:3026\u001B[0m, in \u001B[0;36mcross_entropy\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[0;32m   3024\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3025\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: Target 84 is out of bounds."
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = model.state_dict()\n",
    "    print(f\"train epoch: {epoch+1}----------------\")\n",
    "    for img, label in tqdm(train_loader):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img)\n",
    "        loss = criterion(output, label)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    correct, all_data = 0,0\n",
    "    print(\"\\ntrain_loss : \", running_loss / len(train_loader))\n",
    "    model.eval()\n",
    "    for img, label in test_loader:\n",
    "        with torch.no_grad():\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(img)\n",
    "\n",
    "            correct += torch.sum(torch.argmax(output, dim=1) == label).item()\n",
    "            all_data += len(label)\n",
    "    print(\"val_acc : \", correct / all_data)\n",
    "    if correct / all_data > best_acc:\n",
    "      best_acc = correct / all_data\n",
    "      best_model_wts = model.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                f\"train Epoch: {Epoch} [{batch_idx * len(image)}/{len(train_loader.dataset)}({100. * batch_idx / len(train_loader):.0f}%)]\\tTrain Loss: {loss.item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim=True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Epoch: 1 [0/50000(0%)]\tTrain Loss: 2.3187170028686523\n",
      "train Epoch: 1 [6400/50000(13%)]\tTrain Loss: 1.974173903465271\n",
      "train Epoch: 1 [12800/50000(26%)]\tTrain Loss: 1.949430227279663\n",
      "train Epoch: 1 [19200/50000(38%)]\tTrain Loss: 1.8856143951416016\n",
      "train Epoch: 1 [25600/50000(51%)]\tTrain Loss: 1.6462563276290894\n",
      "train Epoch: 1 [32000/50000(64%)]\tTrain Loss: 1.6108298301696777\n",
      "train Epoch: 1 [38400/50000(77%)]\tTrain Loss: 1.4917792081832886\n",
      "train Epoch: 1 [44800/50000(90%)]\tTrain Loss: 1.5502933263778687\n",
      "\n",
      "[EPOCH: 1]\tTest Loss: 0.0457\tTest Accuracy: 46.18 % \n",
      "\n",
      "train Epoch: 2 [0/50000(0%)]\tTrain Loss: 1.5680345296859741\n",
      "train Epoch: 2 [6400/50000(13%)]\tTrain Loss: 1.1425055265426636\n",
      "train Epoch: 2 [12800/50000(26%)]\tTrain Loss: 1.4482574462890625\n",
      "train Epoch: 2 [19200/50000(38%)]\tTrain Loss: 1.428040623664856\n",
      "train Epoch: 2 [25600/50000(51%)]\tTrain Loss: 1.5430216789245605\n",
      "train Epoch: 2 [32000/50000(64%)]\tTrain Loss: 1.2050542831420898\n",
      "train Epoch: 2 [38400/50000(77%)]\tTrain Loss: 1.0973886251449585\n",
      "train Epoch: 2 [44800/50000(90%)]\tTrain Loss: 1.318777084350586\n",
      "\n",
      "[EPOCH: 2]\tTest Loss: 0.0413\tTest Accuracy: 51.81 % \n",
      "\n",
      "train Epoch: 3 [0/50000(0%)]\tTrain Loss: 1.447056531906128\n",
      "train Epoch: 3 [6400/50000(13%)]\tTrain Loss: 0.9699857831001282\n",
      "train Epoch: 3 [12800/50000(26%)]\tTrain Loss: 1.5999459028244019\n",
      "train Epoch: 3 [19200/50000(38%)]\tTrain Loss: 1.1616674661636353\n",
      "train Epoch: 3 [25600/50000(51%)]\tTrain Loss: 1.108928918838501\n",
      "train Epoch: 3 [32000/50000(64%)]\tTrain Loss: 1.212425708770752\n",
      "train Epoch: 3 [38400/50000(77%)]\tTrain Loss: 1.0619882345199585\n",
      "train Epoch: 3 [44800/50000(90%)]\tTrain Loss: 1.3214360475540161\n",
      "\n",
      "[EPOCH: 3]\tTest Loss: 0.0388\tTest Accuracy: 55.08 % \n",
      "\n",
      "train Epoch: 4 [0/50000(0%)]\tTrain Loss: 1.5132007598876953\n",
      "train Epoch: 4 [6400/50000(13%)]\tTrain Loss: 1.0558247566223145\n",
      "train Epoch: 4 [12800/50000(26%)]\tTrain Loss: 1.128709316253662\n",
      "train Epoch: 4 [19200/50000(38%)]\tTrain Loss: 0.8884413838386536\n",
      "train Epoch: 4 [25600/50000(51%)]\tTrain Loss: 1.0387619733810425\n",
      "train Epoch: 4 [32000/50000(64%)]\tTrain Loss: 1.2822428941726685\n",
      "train Epoch: 4 [38400/50000(77%)]\tTrain Loss: 1.1093487739562988\n",
      "train Epoch: 4 [44800/50000(90%)]\tTrain Loss: 1.1914088726043701\n",
      "\n",
      "[EPOCH: 4]\tTest Loss: 0.0363\tTest Accuracy: 58.14 % \n",
      "\n",
      "train Epoch: 5 [0/50000(0%)]\tTrain Loss: 1.0814989805221558\n",
      "train Epoch: 5 [6400/50000(13%)]\tTrain Loss: 0.9875394701957703\n",
      "train Epoch: 5 [12800/50000(26%)]\tTrain Loss: 1.0328590869903564\n",
      "train Epoch: 5 [19200/50000(38%)]\tTrain Loss: 1.3330539464950562\n",
      "train Epoch: 5 [25600/50000(51%)]\tTrain Loss: 0.9765488505363464\n",
      "train Epoch: 5 [32000/50000(64%)]\tTrain Loss: 1.3053960800170898\n",
      "train Epoch: 5 [38400/50000(77%)]\tTrain Loss: 1.1230601072311401\n",
      "train Epoch: 5 [44800/50000(90%)]\tTrain Loss: 0.9632169604301453\n",
      "\n",
      "[EPOCH: 5]\tTest Loss: 0.0358\tTest Accuracy: 59.18 % \n",
      "\n",
      "train Epoch: 6 [0/50000(0%)]\tTrain Loss: 1.0882210731506348\n",
      "train Epoch: 6 [6400/50000(13%)]\tTrain Loss: 1.2863507270812988\n",
      "train Epoch: 6 [12800/50000(26%)]\tTrain Loss: 1.2586504220962524\n",
      "train Epoch: 6 [19200/50000(38%)]\tTrain Loss: 1.116532802581787\n",
      "train Epoch: 6 [25600/50000(51%)]\tTrain Loss: 0.7596296072006226\n",
      "train Epoch: 6 [32000/50000(64%)]\tTrain Loss: 1.2042490243911743\n",
      "train Epoch: 6 [38400/50000(77%)]\tTrain Loss: 0.729206919670105\n",
      "train Epoch: 6 [44800/50000(90%)]\tTrain Loss: 1.2678126096725464\n",
      "\n",
      "[EPOCH: 6]\tTest Loss: 0.0348\tTest Accuracy: 60.91 % \n",
      "\n",
      "train Epoch: 7 [0/50000(0%)]\tTrain Loss: 1.0795427560806274\n",
      "train Epoch: 7 [6400/50000(13%)]\tTrain Loss: 1.1005656719207764\n",
      "train Epoch: 7 [12800/50000(26%)]\tTrain Loss: 1.1732850074768066\n",
      "train Epoch: 7 [19200/50000(38%)]\tTrain Loss: 1.0315682888031006\n",
      "train Epoch: 7 [25600/50000(51%)]\tTrain Loss: 1.1444092988967896\n",
      "train Epoch: 7 [32000/50000(64%)]\tTrain Loss: 0.8523159623146057\n",
      "train Epoch: 7 [38400/50000(77%)]\tTrain Loss: 0.7680285573005676\n",
      "train Epoch: 7 [44800/50000(90%)]\tTrain Loss: 0.8797897100448608\n",
      "\n",
      "[EPOCH: 7]\tTest Loss: 0.0344\tTest Accuracy: 61.3 % \n",
      "\n",
      "train Epoch: 8 [0/50000(0%)]\tTrain Loss: 1.079163908958435\n",
      "train Epoch: 8 [6400/50000(13%)]\tTrain Loss: 1.1082321405410767\n",
      "train Epoch: 8 [12800/50000(26%)]\tTrain Loss: 1.1269015073776245\n",
      "train Epoch: 8 [19200/50000(38%)]\tTrain Loss: 0.6709139943122864\n",
      "train Epoch: 8 [25600/50000(51%)]\tTrain Loss: 0.9860478639602661\n",
      "train Epoch: 8 [32000/50000(64%)]\tTrain Loss: 1.0185604095458984\n",
      "train Epoch: 8 [38400/50000(77%)]\tTrain Loss: 1.3549479246139526\n",
      "train Epoch: 8 [44800/50000(90%)]\tTrain Loss: 0.9804713726043701\n",
      "\n",
      "[EPOCH: 8]\tTest Loss: 0.0338\tTest Accuracy: 61.36 % \n",
      "\n",
      "train Epoch: 9 [0/50000(0%)]\tTrain Loss: 1.0137219429016113\n",
      "train Epoch: 9 [6400/50000(13%)]\tTrain Loss: 1.0034328699111938\n",
      "train Epoch: 9 [12800/50000(26%)]\tTrain Loss: 0.8604366779327393\n",
      "train Epoch: 9 [19200/50000(38%)]\tTrain Loss: 0.8653223514556885\n",
      "train Epoch: 9 [25600/50000(51%)]\tTrain Loss: 0.7952867150306702\n",
      "train Epoch: 9 [32000/50000(64%)]\tTrain Loss: 0.8518487811088562\n",
      "train Epoch: 9 [38400/50000(77%)]\tTrain Loss: 0.9371556639671326\n",
      "train Epoch: 9 [44800/50000(90%)]\tTrain Loss: 1.030989170074463\n",
      "\n",
      "[EPOCH: 9]\tTest Loss: 0.0353\tTest Accuracy: 60.14 % \n",
      "\n",
      "train Epoch: 10 [0/50000(0%)]\tTrain Loss: 0.8068264126777649\n",
      "train Epoch: 10 [6400/50000(13%)]\tTrain Loss: 0.6816489100456238\n",
      "train Epoch: 10 [12800/50000(26%)]\tTrain Loss: 0.9760589599609375\n",
      "train Epoch: 10 [19200/50000(38%)]\tTrain Loss: 0.6710183620452881\n",
      "train Epoch: 10 [25600/50000(51%)]\tTrain Loss: 1.022810935974121\n",
      "train Epoch: 10 [32000/50000(64%)]\tTrain Loss: 0.7704905867576599\n",
      "train Epoch: 10 [38400/50000(77%)]\tTrain Loss: 1.0720630884170532\n",
      "train Epoch: 10 [44800/50000(90%)]\tTrain Loss: 0.9922505021095276\n",
      "\n",
      "[EPOCH: 10]\tTest Loss: 0.0327\tTest Accuracy: 63.74 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for Epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval=200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(f\"\\n[EPOCH: {Epoch}]\\tTest Loss: {test_loss:.4f}\\tTest Accuracy: {test_accuracy} % \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cad004fe989bd74e5c5aec7ffae436e77866a70d082a239967f7d52eef71aa88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}