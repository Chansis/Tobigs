{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NLP Basic Assignment\n",
    "## NLP 맛보기 - spam.csv를 가지고 유의미한 해석을 도출해주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 필요한 모듈 불러오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data\n",
    "- 보시면 아시다시피 spam.csv는 라벨이 있는 데이터입니다. 물론 7주차 주제가 텍스트 기초인만큼 텍스트만 활용하셔도 되고, 라벨까지 활용하셔서 모델을 돌려보셔도 좋습니다 :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spam = pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\""
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.iloc[5]['v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spam.v1 = spam.v1.apply(lambda x : 0 if x == 'ham' else 1) # spam을 1 ham 을 0으로 변환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.13406317300789664"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.v1.value_counts()[1]/ len(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "test = spam.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'pity    was in mood for that  so   any other suggestions '"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.v2[5569]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanText(x) :\n",
    "    x_ = re.sub('[^a-zA-Z]', ' ', x).lower() # 특수문자 제거 & 소문자\n",
    "    return x_\n",
    "\n",
    "spam.v2 = spam.v2.apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'pity    was in mood for that  so   any other suggestions '"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.v2[5569]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "5147"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.v2.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "test = spam.copy()\n",
    "test1 = spam.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 불용어 처리 및 Tokenizing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 예시 코드 코드\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') # nltk에 저장되어 있는 불용어를 불러온다.\n",
    "english_stops = list(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "english_stops = list(stopwords.words('english'))\n",
    "token = []\n",
    "\n",
    "for sentence in test.v2:\n",
    "    words = word_tokenize(sentence)\n",
    "    words = [word for word in words if word not in english_stops]\n",
    "    token.append(words)\n",
    "\n",
    "test.v2 = token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "0       [go, jurong, point, crazy, available, bugis, n...\n1                          [ok, lar, joking, wif, u, oni]\n2       [free, entry, wkly, comp, win, fa, cup, final,...\n3           [u, dun, say, early, hor, u, c, already, say]\n4          [nah, think, goes, usf, lives, around, though]\n                              ...                        \n5567    [nd, time, tried, contact, u, u, pound, prize,...\n5568                      [b, going, esplanade, fr, home]\n5569                            [pity, mood, suggestions]\n5570    [guy, bitching, acted, like, interested, buyin...\n5571                                   [rofl, true, name]\nName: v2, Length: 5572, dtype: object"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.v2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "with open('test.txt', 'w', encoding= 'utf8') as f :\n",
    "    for row in test1.itertuples() :\n",
    "        f.write(row.v2 + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "sp = SentencePieceBPETokenizer()\n",
    "sp.train(files = 'test.txt', min_frequency = 1, vocab_size = 2000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "sp = SentencePieceBPETokenizer()\n",
    "sp.train(files = 'test.txt', min_frequency = 1, vocab_size = 2000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "2000"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.get_vocab_size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "word2idx = sp.get_vocab()\n",
    "idx2word = {j:i for i,j in word2idx.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def getlist(x) :\n",
    "    tokens = sp.encode(x).tokens\n",
    "    results = [token for token in tokens if token != '▁'] #의미없는 띄어쓰기는 제거 해준다.\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "test1.v2 = test1.v2.apply(getlist)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "0       [▁go, ▁until, ▁j, ur, ong, ▁point, ▁crazy, ▁av...\n1                [▁ok, ▁lar, ▁jo, king, ▁wif, ▁u, ▁on, i]\n2       [▁free, ▁entry, ▁in, ▁a, ▁wkly, ▁comp, ▁to, ▁w...\n3       [▁u, ▁dun, ▁say, ▁so, ▁early, ▁hor, ▁u, ▁c, ▁a...\n4       [▁nah, ▁i, ▁don, ▁t, ▁think, ▁he, ▁goes, ▁to, ...\n                              ...                        \n5567    [▁this, ▁is, ▁the, ▁nd, ▁time, ▁we, ▁have, ▁tr...\n5568    [▁will, ▁b, ▁going, ▁to, ▁es, pl, an, ade, ▁fr...\n5569    [▁p, ity, ▁was, ▁in, ▁mo, od, ▁for, ▁that, ▁so...\n5570    [▁the, ▁guy, ▁did, ▁some, ▁bit, ch, ing, ▁but,...\n5571           [▁ro, f, l, ▁its, ▁true, ▁to, ▁its, ▁name]\nName: v2, Length: 5572, dtype: object"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.v2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Embedding\n",
    "\n",
    "- 수업에서 다룬 임베딩 방법에는 One-hot encoding, CBOW, Skip-gram, GloVe, FastText가 있었습니다. 다양한 시도와 '비교' 결과를 함께 적어주세요! 파라미터를 조정해가는 과정도 해석에 도움이 될 수 있겠죠 :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv1 = CountVectorizer(max_features= 500, stop_words = 'english', lowercase= False)\n",
    "cv2 = CountVectorizer(max_features= 500, stop_words = 'english', lowercase= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens_ham = sum(test[test.v1 == 0]['v2'], []) # CountVectorizer는 리스트 형태로 값이 들어와야 하기 때문에 이중 리스트를 flatten 시켜준다.\n",
    "tokens_spam = sum(test[test.v1 == 1]['v2'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "tdm_ham = cv1.fit_transform(tokens_ham)\n",
    "tdm_spam = cv2.fit_transform(tokens_spam)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 본인이 도출해낸 해석을 적어주세요!\n",
    "\n",
    "- 유사도, Wordcloud, 이진 분류 모델, Plot 뭐든 상관없으니 분명하고 인상적인 해석을 적어주시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}